{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13376c37",
      "metadata": {},
      "source": [
        "### Runtime setup\n",
        "The following envs enable stable retries and quiet streaming.\n",
        "\n",
        "- `SCILLM_DISABLE_AIOHTTP=1` (httpx-only async stability)\n",
        "- `SCILLM_FORCE_HTTPX_STREAM=1`\n",
        "- `LITELLM_MAX_RETRIES=3`, `LITELLM_RETRY_AFTER=1`, `LITELLM_TIMEOUT=45`\n",
        "- Requires `tenacity` installed for backoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2740e2c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ.setdefault('SCILLM_DISABLE_AIOHTTP','1')\n",
        "os.environ.setdefault('SCILLM_FORCE_HTTPX_STREAM','1')\n",
        "os.environ.setdefault('LITELLM_MAX_RETRIES','3')\n",
        "os.environ.setdefault('LITELLM_RETRY_AFTER','1')\n",
        "os.environ.setdefault('LITELLM_TIMEOUT','45')\n",
        "try:\n",
        "    import tenacity  # noqa: F401\n",
        "    print('tenacity: ok')\n",
        "except Exception:\n",
        "    print('tenacity missing — run: pip install tenacity')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ff1349",
      "metadata": {},
      "source": [
        "# Chutes — OpenAI-Compatible\n",
        "\n",
        "        Minimal chat using the OpenAI-compatible path."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36198ab",
      "metadata": {},
      "source": [
        "## 1) Sync completion\n",
        "\n",
        "        Minimal, blocking call using `scillm.completion`. Good for quick sanity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fc68345",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from scillm import completion\n",
        "resp = completion(\n",
        "  model=os.environ['CHUTES_MODEL'],\n",
        "  api_base=os.environ['CHUTES_API_BASE'],\n",
        "  api_key=os.environ['CHUTES_API_KEY'],\n",
        "  custom_llm_provider='openai_like',\n",
        "  messages=[{'role':'user','content':'Say OK'}],\n",
        "  max_tokens=8,\n",
        "  temperature=0,\n",
        ")\n",
        "print(resp.choices[0].message.get('content',''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Router + Fallbacks (Text) — Recommended\n",
        "Use Router with a shared model_name for primary+alternates. Capacity (429/503) cools down the failing deployment and tries the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from litellm import Router\n",
        "# Env defaults (set once):\n",
        "# SCILLM_CHUTES_CANONICALIZE_OPENAI_AUTH=1\n",
        "# LITELLM_MAX_RETRIES=3 LITELLM_RETRY_AFTER=2\n",
        "# SCILLM_COOLDOWN_429_S=120 SCILLM_RATE_LIMIT_QPS=2\n",
        "router_text = Router(model_list=[\n",
        "  {\"model_name\": \"chutes/text\",\n",
        "   \"litellm_params\": {\"custom_llm_provider\": \"openai_like\",\n",
        "     \"model\": os.environ['CHUTES_TEXT_MODEL'],\n",
        "     \"api_base\": os.environ['CHUTES_API_BASE'],\n",
        "     \"api_key\": os.environ['CHUTES_API_KEY'],\n",
        "     \"order\": 1}},\n",
        "  {\"model_name\": \"chutes/text\",\n",
        "   \"litellm_params\": {\"custom_llm_provider\": \"openai_like\",\n",
        "     \"model\": os.environ.get('CHUTES_TEXT_MODEL_ALT1',''),\n",
        "     \"api_base\": os.environ['CHUTES_API_BASE'],\n",
        "     \"api_key\": os.environ['CHUTES_API_KEY'],\n",
        "     \"order\": 2}},\n",
        "])\n",
        "out = router_text.completion(\n",
        "  model='chutes/text',\n",
        "  messages=[{\"role\":\"user\",\"content\":'Return only {\\\"ok\\\": true} as JSON.'}],\n",
        "  response_format={\"type\":\"json_object\"},\n",
        ")\n",
        "print(out.choices[0].message.get('content',''))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b873a520",
      "metadata": {},
      "source": [
        "### Auto Router (one‑liner) — Recommended for fallbacks\n",
        "Why: eliminates manual ALT1/ALT2. Discovers models from your Chutes `/v1/models`, filters by capability (text/VLM; `require_json`/`require_tools`), and ranks by availability/utilization. You get automatic failover on capacity (429/503) using your standard env backoff settings.\n",
        "\n",
        "How: set env once, then build a Router with a single call.\n",
        "- `SCILLM_CHUTES_CANONICALIZE_OPENAI_AUTH=1`\n",
        "- `LITELLM_MAX_RETRIES=3 LITELLM_RETRY_AFTER=2`\n",
        "- `SCILLM_COOLDOWN_429_S=120 SCILLM_RATE_LIMIT_QPS=2`\n",
        "- optional: `SCILLM_DISABLE_AIOHTTP=1 LITELLM_TIMEOUT=45`\n",
        "\n",
        "Notes:\n",
        "- Honors multiple bases via `CHUTES_API_BASE_1/CHUTES_API_KEY_1` (and _2, _3...) if present.\n",
        "- Override odd model capabilities with `SCILLM_MODEL_CAPS_JSON`.\n",
        "- Use `kind=\"text\"` or `kind=\"vlm\"`; add `require_json=True` to enforce JSON mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb92ec8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from scillm.extras import auto_router_from_env\n",
        "\n",
        "# Build a text Router with automatic fallbacks\n",
        "router_text = auto_router_from_env(kind='text', require_json=True)\n",
        "out = router_text.completion(\n",
        "  model='chutes/text',\n",
        "  messages=[{\"role\":\"user\",\"content\":'Return only {\\\"ok\\\": true} as JSON.'}],\n",
        "  response_format={\"type\":\"json_object\"},\n",
        ")\n",
        "print(out.choices[0].message.get('content',''))\n",
        "\n",
        "# (Optional) Vision:\n",
        "# router_vlm = auto_router_from_env(kind='vlm', require_json=True)\n",
        "# out_v = router_vlm.completion(\n",
        "#   model='chutes/vlm',\n",
        "#   messages=[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":'Return only {\\\"ok\\\": true} as JSON.'}]}],\n",
        "#   response_format={\"type\":\"json_object\"},\n",
        "# )\n",
        "# print(out_v.choices[0].message.get('content',''))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
