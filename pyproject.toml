##############################
# scillm – uv / hatch config #
##############################

[project]
name = "scillm"
version = "1.77.2"
description = "SciLLM — a scientific/engineering‑focused fork of LiteLLM (OpenAI‑compatible gateway + optional modules)"
readme = "README.md"
authors = [{name = "Graham Anderson", email = "graham@grahama.co"}]
license = {text = "MIT"}
requires-python = ">=3.8.1,<4.0, !=3.9.7"
dependencies = [
  "fastuuid>=0.12.0",
  "httpx>=0.23.0",
  "openai>=1.99.5",
  "python-dotenv>=0.2.0",
  "tiktoken>=0.7.0",
  "importlib-metadata>=6.8.0",
  "tokenizers",
  "click",
  "json-repair>=0.51.0; python_version >= '3.10'",
  "jinja2>=3.1.2",
  "aiohttp>=3.10",
  "pydantic>=2.5.0",
  "jsonschema>=4.22.0",
  "pondpond>=1.4.1",
  "pytest-html>=4.1.1",
]

[project.urls]
Homepage = "https://github.com/grahama1970/scillm"
Repository = "https://github.com/grahama1970/scillm"
Upstream = "https://github.com/BerriAI/litellm"
Documentation = "https://docs.litellm.ai"

[project.optional-dependencies]
proxy = [
  "gunicorn>=23.0.0",
  "uvicorn>=0.29.0",
  "uvloop>=0.21.0; sys_platform != 'win32'",
  "fastapi>=0.115.5",
  "backoff",
  "pyyaml>=6.0.1",
  "rq",
  "orjson>=3.9.7",
  "apscheduler>=3.10.4",
  "fastapi-sso>=0.16.0",
  "PyJWT>=2.8.0",
  "python-multipart>=0.0.18",
  "cryptography",
  "pynacl>=1.5.0",
  "websockets>=13.1.0",
  "boto3==1.36.0",
  "azure-identity>=1.15.0",
  "azure-storage-blob>=12.25.1",
  "mcp>=1.10.0; python_version >= '3.10'",
  "litellm-proxy-extras==0.2.18",
  "litellm-enterprise==0.1.20",
  "rich==13.7.1",
  "polars>=1.31.0; python_version >= '3.10'",
]
extra_proxy = [
  "prisma==0.11.0",
  "azure-identity>=1.15.0",
  "azure-keyvault-secrets>=4.8.0",
  "google-cloud-kms>=2.21.3",
  "google-cloud-iam>=2.19.1",
  "resend>=0.8.0",
  "redisvl>=0.4.1; python_version >= '3.9' and python_version < '3.14'",
]
utils = ["numpydoc"]
caching = ["diskcache>=5.6.1"]
mlflow = ["mlflow>3.1.4; python_version >= '3.10'"]

[project.scripts]
litellm = "litellm:run_server"
litellm-proxy = "litellm.proxy.client.cli:cli"
scillm = "litellm:run_server"
scillm-proxy = "litellm.proxy.client.cli:cli"

[build-system]
requires = ["hatchling>=1.24.0"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
include = [
  "litellm/**",
  "litellm/py.typed",
  "scillm/**",
]

[dependency-groups]
dev = [
  "flake8>=6.1.0",
  "black>=23.12.0",
  "mypy>=1.0",
  "pytest>=7.4.3",
  "pytest-mock>=3.12.0",
  "pytest-asyncio>=0.21.1",
  "requests-mock>=1.12.1",
  "responses>=0.25.7",
  "respx>=0.22.0",
  "ruff>=0.1.0",
  "types-requests",
  "types-setuptools",
  "types-redis",
  "types-PyYAML",
  "opentelemetry-api==1.25.0",
  "opentelemetry-sdk==1.25.0",
  "opentelemetry-exporter-otlp==1.25.0",
  "langfuse>=2.45.0",
  "fastapi-offline>=1.7.3",
]
proxy-dev = [
  "prisma==0.11.0",
  "hypercorn>=0.15.0",
  "prometheus-client==0.20.0",
  "opentelemetry-api==1.25.0",
  "opentelemetry-sdk==1.25.0",
  "opentelemetry-exporter-otlp==1.25.0",
  "azure-identity>=1.15.0",
]

[tool.isort]
profile = "black"

[tool.commitizen]
version = "1.77.2"
version_files = ["pyproject.toml:^version"]

[tool.mypy]
plugins = "pydantic.mypy"
