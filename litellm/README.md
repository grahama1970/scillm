<p align="center">
  <img src="../local/artifacts/logo/SciLLM_balanced_outlined.svg" alt="SciLLM" width="140" />
  <br/>
  <img src="../local/artifacts/logo/SciLLM_icon.svg" alt="SciLLM Icon" width="44" />
  <br/>
  <em>Balanced wordmark (default) + icon (logoâ€‘only). The favicon (.ico) uses the icon only.</em>
</p>

<h1 align="center">ğŸ”¬ SciLLM â€” a Scientific/Engineering fork of LiteLLM</h1>

<p align="center">
  <a href="https://github.com/grahama1970/scillm/actions/workflows/nightly-parity-stress.yml"><img src="https://github.com/grahama1970/scillm/actions/workflows/nightly-parity-stress.yml/badge.svg" alt="SciLLM: Nightly Parity & Stress"></a>
  <a href="https://github.com/grahama1970/scillm/actions/workflows/weekly-streaming-stress.yml"><img src="https://github.com/grahama1970/scillm/actions/workflows/weekly-streaming-stress.yml/badge.svg" alt="SciLLM: Weekly Streaming Stress"></a>
  <a href="https://github.com/grahama1970/scillm/actions/workflows/manual-stress.yml"><img src="https://img.shields.io/badge/SciLLM%20Manual%20Stress-%E2%86%92-blue" alt="SciLLM: Manual Stress"></a>
</p>

<p align="center"><i>APIâ€‘compatible with LiteLLM. Adds optional modules for: (1) formal methods via a prover bridge (â€œCertainlyâ€, Lean4 in beta), (2) code strategy orchestration with dynamic scoring (â€œCodeWorldâ€), and (3) small live agent flows (miniâ€‘agent, codexâ€‘agent). See the root <a href="../QUICK_START.md">QUICK_START.md</a> and scenarios/ for runnable demos. Enable with SCILLM_ENABLE_* or LITELLM_ENABLE_*.</i></p>

## TL;DR (30 seconds)

```bash
# Bring up bridges + proxy + deps (from repo root)
docker compose -f deploy/docker/compose.scillm.stack.yml up --build -d

# Two live scenarios (skipâ€‘friendly)
python ../scenarios/codeworld_judge_live.py
LITELLM_ENABLE_CERTAINLY=1 CERTAINLY_BRIDGE_BASE=http://127.0.0.1:8787 \
  python ../scenarios/certainly_router_release.py
```

---

## Why SciLLM

SciLLM targets practitioners who need reproducible, inspectable endâ€‘toâ€‘end workflows that combine LLMs with verifiable computation.

- Who itâ€™s for
  - Scientists/engineers building proofâ€‘ofâ€‘concepts around formal verification (Lean4 today), algorithm selection, and agent toolâ€‘use loops.
  - Teams who want OpenAIâ€‘compatible ergonomics, localâ€‘first bringâ€‘up (Docker), and â€œone way to greenâ€ readiness gates.
  - Educators/tinkerers who want runnable scenarios and artifacts they can inspect.

- What you get
  - Any LLM model LiteLLM supports (local or cloud) behind the same OpenAIâ€‘style surface.
  - Certainly (Lean4 umbrella, beta): convert naturalâ€‘language + structured requirements to Lean4, return proofs or structured guidance/diagnostics.
  - CodeWorld: run multiple concurrent strategies safely; add dynamic scoring; judge/rank winners.
  - codexâ€‘agent: codeâ€‘centric agent surface; run multiâ€‘iteration plans and call MCP tools via your sidecar.
  - miniâ€‘agent: tiny deterministic agent for quick toolâ€‘use experiments.
  - Reproducibility by design: perâ€‘run artifacts (run_id, request_id, item_ids, session/track) and strict readiness gates.

---

## Modules at a Glance

| Module | What it is | When to use | How it works |
| --- | --- | --- | --- |
| Certainly (Lean4 umbrella) | Prover bridge under a stable provider alias | Batchâ€‘check obligations/lemmas; keep client code stable while swapping backends | Router posts `{messages, items, options}` to Lean4 bridge; returns `summary + results + run_manifest` with diagnostics |
| CodeWorld | Strategy orchestrator with dynamic judge | Compare multiple algorithms on the same inputs; evaluate under your metrics; rank winners | Execute strategies with RLIMITs; optional noâ€‘net; dynamic Python `score()`; builtâ€‘in weighted/lex judge |
| miniâ€‘agent | Tiny agent loop for toolâ€‘use | Local, deterministic experiments with Python/Rust/Go/JS tools | Inâ€‘process shim or Docker tools sidecar; emits parsed tool calls and metrics |
| codexâ€‘agent | Codeâ€‘oriented agent provider | Sidecar/HTTP codex flows exposed via LiteLLM | Envâ€‘gated provider; OpenAIâ€‘compatible; integrates via Router |

---

## Realâ€‘World Scenarios

- Multiâ€‘heuristic selection (CodeWorld): package DP/heuristics as variants; run both; supply a domainâ€‘specific `score()`; judge with correctness/speed/brevity; keep winner + provenance.
- Spec compliance verification (Certainly): send `messages + lean4_requirements` (or canonical `items`); get proof results with stable item_ids and diagnostics; rerun with the same manifest.
- Innerâ€‘loop bug fix (miniâ€‘agent): run tool invocations deterministically; capture final answer + telemetry.
- Code refactor planning (codexâ€‘agent): codeâ€‘centric agent via OpenAIâ€‘compatible provider; healthâ€‘checkable sidecar.

See <a href="../QUICK_START.md">QUICK_START.md</a> for runnable commands.

---

## Installation and Compatibility

- pip install: `pip install litellm` (SciLLM remains APIâ€‘compatible with LiteLLM)
- Env flags (preferred): `SCILLM_ENABLE_*` (aliases supported: `LITELLM_ENABLE_*`)
- CLI aliases: `scillm`, `scillm-proxy` (mirrors LiteLLM commands)
- Deployment: `deploy/docker/compose.scillm.stack.yml` provides bridges + local tooling

---

## Quick Start

See the root quick start: <a href="../QUICK_START.md">QUICK_START.md</a>

---

<details>
  <summary>Logo variants</summary>
  <p>
    <img src="../local/artifacts/logo/SciLLM_balanced_outlined.svg" alt="SciLLM Balanced (default, outlined)" height="36" />
    &nbsp;&nbsp;
    <img src="../local/artifacts/logo/SciLLM_icon.svg" alt="SciLLM Icon" height="36" />
    &nbsp;&nbsp;
    <img src="../local/artifacts/logo/SciLLM_balanced_dark.svg" alt="SciLLM Balanced Dark" height="36" />
    &nbsp;&nbsp;
    <img src="../local/artifacts/logo/SciLLM_balanced_mono.svg" alt="SciLLM Balanced Mono" height="36" />
  </p>
  <p>Use <code>make logo-export</code> to produce outlined SVGs and favicons in <code>local/artifacts/logo/</code>. The generated <code>favicon.ico</code> uses the icon only (no text).</p>
</details>
