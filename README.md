<p align="center">
  <!-- Use outlined balanced logo for pixel-consistent rendering across systems -->
  <img src="local/artifacts/logo/SciLLM_balanced_outlined.svg" alt="SciLLM" width="140" />
  <br/>
  <img src="local/artifacts/logo/SciLLM_icon.svg" alt="SciLLM Icon" width="44" />
  <br/>
  <em>Balanced wordmark (default) + icon (logo‚Äëonly). The favicon (.ico) should use the icon only, no text.</em>
 </p>
<h1 align="center">üî¨ SciLLM ‚Äî Scientific/Engineering fork of LiteLLM</h1>
<h4 align="center"><a href="https://docs.litellm.ai/docs/simple_proxy" target="_blank">Proxy Server (LLM Gateway)</a> | <a href="https://docs.litellm.ai/docs/hosted" target="_blank"> Hosted Proxy (Preview)</a> | <a href="https://docs.litellm.ai/docs/enterprise"target="_blank">Enterprise Tier</a></h4>

<p align="center">
  <a href="https://github.com/grahama1970/scillm/actions/workflows/nightly-parity-stress.yml"><img src="https://github.com/grahama1970/scillm/actions/workflows/nightly-parity-stress.yml/badge.svg" alt="SciLLM: Nightly Parity & Stress"></a>
  <a href="https://github.com/grahama1970/scillm/actions/workflows/weekly-streaming-stress.yml"><img src="https://github.com/grahama1970/scillm/actions/workflows/weekly-streaming-stress.yml/badge.svg" alt="SciLLM: Weekly Streaming Stress"></a>
  <a href="https://github.com/grahama1970/scillm/actions/workflows/manual-stress.yml"><img src="https://img.shields.io/badge/SciLLM%20Manual%20Stress-%E2%86%92-blue" alt="SciLLM: Manual Stress"></a>
</p>

<h4 align="center">
    <a href="https://pypi.org/project/litellm/" target="_blank">
        <img src="https://img.shields.io/pypi/v/litellm.svg" alt="PyPI Version">
    </a>
    <a href="https://www.ycombinator.com/companies/berriai">
        <img src="https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square" alt="Y Combinator W23">
    </a>
    <a href="https://wa.link/huol9n">
        <img src="https://img.shields.io/static/v1?label=Chat%20on&message=WhatsApp&color=success&logo=WhatsApp&style=flat-square" alt="Whatsapp">
    </a>
    <a href="https://discord.gg/wuPM9dRgDw">
        <img src="https://img.shields.io/static/v1?label=Chat%20on&message=Discord&color=blue&logo=Discord&style=flat-square" alt="Discord">
    </a>
    <a href="https://www.litellm.ai/support">
        <img src="https://img.shields.io/static/v1?label=Chat%20on&message=Slack&color=black&logo=Slack&style=flat-square" alt="Slack">
    </a>
</h4>

<p align="center"><i>This fork remains API‚Äëcompatible with LiteLLM while adding optional modules for formal methods (Lean4, exposed as "Certainly"), code orchestration (CodeWorld), and live agent flows. See QUICKSTART.md and scenarios/ for runnable demos. Use SCILLM_ENABLE_* or LITELLM_ENABLE_* flags to enable modules.</i></p>

<p><b>Why SciLLM?</b> SciLLM adds specialized infrastructure for theorem proving (Lean4 / ‚ÄúCertainly‚Äù), code orchestration (CodeWorld + MCTS), and agent/tool experimentation (mini‚Äëagent, codex‚Äëagent) ‚Äî while retaining LiteLLM API compatibility.</p>

<blockquote>
<b>Environment Prefix Preference</b><br/>
Use <code>SCILLM_</code> variables (e.g. <code>SCILLM_ENABLE_CODEX_AGENT=1</code>). Legacy <code>LITELLM_</code> aliases remain for backward compatibility.<br/>
<b>Model IDs</b>: Replace <code><MODEL_ID></code> placeholders with a real ID from <code>GET $CODEX_AGENT_API_BASE/v1/models</code> (older examples used <code>gpt-5</code> illustratively).
</blockquote>

## TL;DR (30 seconds)

```bash
# Bring up bridges + proxy + deps
docker compose -f deploy/docker/compose.scillm.stack.yml up --build -d

# Two live scenarios (skip‚Äëfriendly)
python scenarios/codeworld_judge_live.py
LITELLM_ENABLE_CERTAINLY=1 CERTAINLY_BRIDGE_BASE=http://127.0.0.1:8787 \
  python scenarios/certainly_router_release.py
```

### Happy Path for Project Agents (codex‚Äëagent)
- Zero‚Äëambiguity checklist (copy/paste)
  - Choose ONE runtime:
    - Local: `uvicorn litellm.experimental_mcp_client.mini_agent.agent_proxy:app --host 127.0.0.1 --port 8788`
    - Docker: `docker compose -f local/docker/compose.agents.yml up --build -d codex-sidecar`
  - Set base (no `/v1`): `export CODEX_AGENT_API_BASE=http://127.0.0.1:8788` (or `:8077` for Docker)
  - Map OpenAI envs (for HTTP clients):
    - `export OPENAI_BASE_URL="$CODEX_AGENT_API_BASE"`
    - `export OPENAI_API_KEY="${CODEX_AGENT_API_KEY:-none}"`
  - Discover a model id: `curl -sS "$CODEX_AGENT_API_BASE/v1/models" | jq -r '.data[].id'`
  - Quick HTTP (high reasoning):
    `curl -sS "$CODEX_AGENT_API_BASE/v1/chat/completions" -H 'Content-Type: application/json' -d '{"model":"gpt-5","reasoning":{"effort":"high"},"messages":[{"role":"user","content":"ping"}]}' | jq -r '.choices[0].message.content'`
- Router call: `completion(model="<MODEL_ID>", custom_llm_provider="codex-agent", api_base=$CODEX_AGENT_API_BASE, messages=[...], reasoning_effort="high")`
- Model-only UX: `completion(model="codex-agent/gpt-5", api_base=$CODEX_AGENT_API_BASE, messages=[...], response_format={"type":"json_object"}, temperature=1)`
- Optional cache: `from litellm.extras import initialize_litellm_cache; initialize_litellm_cache()`

MCTS (CodeWorld) one‚ÄëPOST live check
- Start bridge: `PYTHONPATH=src uvicorn codeworld.bridge.server:app --port 8888 --log-level warning`
- One‚ÄëPOST autogen+MCTS: `CODEWORLD_BASE=http://127.0.0.1:8888 curl -sS "$CODEWORLD_BASE/bridge/complete" -H 'Content-Type: application/json' -d '{"messages":[{"role":"user","content":"Autogenerate then search"}],"items":[{"task":"t","context":{}}],"provider":{"name":"codeworld","args":{"strategy":"mcts","strategy_config":{"autogenerate":{"enabled":true,"n":3},"rollouts":24,"depth":6,"uct_c":1.25}}}}' | jq '.run_manifest.mcts_stats'`
- Or just run: `CODEWORLD_BASE=http://127.0.0.1:8888 make mcts-live`

Troubleshooting
- 404 ‚Üí wrong model id; use one from `/v1/models`.
- 400/502 (sidecar) ‚Üí upstream provider not wired; enable echo or add credentials.
- Base includes `/v1` ‚Üí remove it; use the base only.

Doctor (one-shot): `make codex-agent-doctor`

---
## Quick Links
| Topic | File |
|-------|------|
| Feature matrix & patterns | [FEATURES.md](FEATURES.md) |
| Multi‚ÄëSurface Quickstart | [QUICKSTART.md](QUICKSTART.md) |
| Lean4 / Certainly specifics | `scenarios/lean4_*`, `scenarios/certainly_*` |
| Parallel fan‚Äëout example | `feature_recipes/parallel_acompletions.py` |
| MCTS & autogen | `scenarios/mcts_codeworld_demo.py` |
| Retry guide | `docs/guide/RATE_LIMIT_RETRIES.md` |
| Batch helpers | `docs/guide/batch_helpers.md` |

## Security & Isolation (Central Reference)
| Area | Guidance |
|------|----------|
| codex‚Äëagent echo mode | Development only (<code>CODEX_SIDECAR_ECHO=1</code>). Disable before real credentials. |
| CodeWorld sandbox | Process RLIMITs + optional network namespace; containerize for production isolation. |
| Mini‚ÄëAgent images | Detected but not processed (text‚Äëonly semantics). |
| Mini‚ÄëAgent traces | Enable with <code>MINI_AGENT_STORE_TRACES=1</code> + <code>MINI_AGENT_STORE_PATH</code> to capture JSONL transcripts. |
| Output trust | Treat LLM outputs as untrusted; verify via proofs, scoring, deterministic tests. |
| Parallel fan‚Äëout | Prefer ordered <code>parallel_acompletions</code>; <code>parallel_as_completed</code> is experimental/unordered. |

---

## codex‚Äëagent (OpenAI‚Äëcompatible) ‚Äî Local or Docker

Use either local (mini‚Äëagent) or Docker (sidecar). Both expose the same API; do NOT append `/v1` to the base.

- Start shim (default 127.0.0.1:8788):
  - `uvicorn litellm.experimental_mcp_client.mini_agent.agent_proxy:app --host 127.0.0.1 --port 8788`
- Env (set *before* importing Router):
  - `export SCILLM_ENABLE_CODEX_AGENT=1` (alias: `LITELLM_ENABLE_CODEX_AGENT=1`)
  - `export CODEX_AGENT_API_BASE=http://127.0.0.1:8788`  # no `/v1`
  - `# export CODEX_AGENT_API_KEY=...` (usually unset for local)
- Verify:
  - `curl -sSf http://127.0.0.1:8788/healthz`
  - `curl -sS  http://127.0.0.1:8788/v1/models | jq .`
  - (Optional) High reasoning chat:
    ```bash
    MODEL_ID=$(curl -sS $CODEX_AGENT_API_BASE/v1/models | jq -r '.data[0].id')
    curl -sS -H 'content-type: application/json' \
      -d "{\"model\":\"$MODEL_ID\",\"reasoning\":{\"effort\":\"high\"},\"messages\":[{\"role\":\"user\",\"content\":\"say hello\"}]}" \
      $CODEX_AGENT_API_BASE/v1/chat/completions | jq -r '.choices[0].message.content'
    ```
- Router usage (strict JSON):
  ```python
  from litellm import Router
  import os
  router = Router()
  MODEL_ID = os.getenv("CODEX_MODEL_ID","<MODEL_ID>")
  out = router.completion(
      model=MODEL_ID,
      custom_llm_provider="codex-agent",
      messages=[{"role":"user","content":"Return STRICT JSON only: {\"ok\":true}"}],
      response_format={"type":"json_object"},
      retry_enabled=True,
      honor_retry_after=True
  )
  print(out.choices[0].message["content"])  # JSON
  ```

Docker option: `docker compose -f local/docker/compose.agents.yml up --build -d` exposes mini‚Äëagent on `127.0.0.1:8788` and the codex sidecar on `127.0.0.1:8077`. Point `CODEX_AGENT_API_BASE` at the one you want (no `/v1`).

codex‚Äëagent base rule and endpoints
- Set `CODEX_AGENT_API_BASE` WITHOUT `/v1`; the provider appends `/v1/chat/completions`.
- Sidecar (8077) and mini‚Äëagent (8788) expose:
  - `GET /healthz`, `GET /v1/models` (stub), `POST /v1/chat/completions` (content is always a string).

Auth for sidecar (non‚Äëecho mode)
- The compose file enables echo by default (`CODEX_SIDECAR_ECHO=1`). To use real credentials, disable echo and mount your auth file:
  - Edit `local/docker/compose.agents.yml` codex‚Äësidecar service and remove `CODEX_SIDECAR_ECHO` or set to `"0"`.
  - Mount your credentials: `- ${HOME}/.codex/auth.json:/root/.codex/auth.json:ro`.
  - Verify: `python debug/check_codex_auth.py --container litellm-codex-agent` (exit 0 means present).

Debug probes (copy/paste)
- Mini‚Äëagent: `python debug/verify_mini_agent.py` (use `--local` to spawn a local uvicorn on 8789)
- Codex sidecar: `python debug/verify_codex_agent_docker.py` (adds `--start` to compose‚Äëup)
- Parallel Router ‚Üí codex‚Äëagent echo: `python debug/codex_parallel_probe.py` (prints `content` and `scillm_router`)

Optional Router mapping for judge
```python
from litellm import Router
r = Router(model_list=[{"model_name":"gpt-5","litellm_params":{"model":"gpt-5","custom_llm_provider":"codex-agent","api_base":os.getenv("CODEX_AGENT_API_BASE"),"api_key":os.getenv("CODEX_AGENT_API_KEY")}}])
```

### Judge (parameter‚Äëfirst; completion and helper forms)

- Completion (no envs required besides base):
  ```python
  from scillm import completion
  base = "http://127.0.0.1:8089"
  msgs=[
    {"role":"system","content":"Return STRICT JSON only: {best_id:string, rationale_short:string}."},
    {"role":"user","content":"A vs B ‚Äî pick one and say why (short)."},
  ]
  r = completion(model="gpt-5", custom_llm_provider="codex-agent", api_base=base,
                 messages=msgs, response_format={"type":"json_object"},
                 temperature=1, allowed_openai_params=["reasoning","reasoning_effort"], reasoning_effort="medium")
  print(r.choices[0].message["content"])  # strict JSON
  ```

- Minimal helper (direct HTTP, no Router):
  ```python
  from scillm.extras.codex import chat
  res = chat(messages=msgs, model="gpt-5", base=base,
             response_format={"type":"json_object"}, temperature=1, reasoning_effort="medium")
  print(res["choices"][0]["message"]["content"])  # strict JSON
  ```

### Codex‚ÄëCloud (Experimental)

Use codex-ts-sdk to run best‚Äëof‚ÄëN remote code tasks as an alternative to local codex‚Äëagent:

- One‚Äëtime install: `cd scillm/extras/js && npm install`
- Enable + run smoke:
  ```bash
  export SCILLM_EXPERIMENTAL_CODEX_CLOUD=1
  export CODEX_CLOUD_API_KEY=...   # or OPENAI_API_KEY
  # Optional: export CODEX_CLOUD_BASE_URL=... CODEX_CLOUD_ENV=prod
  python debug/live_best_of_n_and_judge.py
  ```
- Notes:
  - Produces a simple variants dict with a diff (PoC). We can expand to per‚Äëattempt variants.
  - Chat/completions for `codex-cloud` are intentionally not implemented yet; use helpers only.

### Retry metadata (429 backoff)
Set:
```
SCILLM_RETRY_META=1
SCILLM_LOG_JSON=1
```
Result snippet:
```json
"additional_kwargs": {
  "router": {
    "retries": {
      "attempts": 2,
      "total_sleep_s": 5.4,
      "last_retry_after_s": 3.0
    }
  }
}
```

## Rate Limiting & Retries (429)

### Chutes mode (QPS pacing)
If you target an OpenAI‚Äëcompatible gateway with ~180 RPM limits (e.g., Chutes), you can enable gentle, process‚Äëlocal pacing:

```bash
export SCILLM_RATE_LIMIT_QPS=2.5   # ~150 RPM average
export SCILLM_COOLDOWN_429_S=120   # cool‚Äëdown window after a 429
```

Pair this with a low runner concurrency (e.g., MAX_WORKERS=2) to avoid bursts.


For long unattended runs that encounter provider 429s, SciLLM‚Äôs Router supports opt‚Äëin retrier logic (Retry‚ÄëAfter awareness, exponential jitter backoff, budgets, callbacks). See docs/guide/RATE_LIMIT_RETRIES.md for env and per‚Äëcall examples.

## Warm‚Äëups in CI (Chutes/Runpod)

Some OpenAI‚Äëcompatible providers benefit from a short daily warm‚Äëup to avoid first‚Äëtoken latency spikes. This fork ships skip‚Äëfriendly warm‚Äëup scripts and a strict composite gate you can opt into. To enable strict warm‚Äëups in GitHub Actions, add a job step like:

```yaml
jobs:
  readiness-live:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - name: Install deps
        run: pip install -U -r requirements.txt
      - name: Live readiness (strict warm-ups)
        env:
          READINESS_LIVE: '1'
          STRICT_READY: '1'
          STRICT_WARMUPS: '1'
          CHUTES_API_KEY: ${{ secrets.CHUTES_API_KEY }}
          RUNPOD_API_KEY: ${{ secrets.RUNPOD_API_KEY }}
        run: |
          make project-ready-live
```

- Strict gate details are defined in `readiness.yml` as `chutes_warmup_strict`, `runpod_warmup_strict`, and `warmups_strict_all`. For a quick manual probe, you can also run:

```bash
python scenarios/provider_warmup_probe.py --provider chutes --model "$LITELLM_DEFAULT_MODEL"
python scenarios/provider_warmup_probe.py --provider runpod --model "$LITELLM_DEFAULT_MODEL"
```

If `STRICT_WARMUPS` is not set, warm‚Äëups remain optional and will not fail the job.

## Why SciLLM

SciLLM exists as an experimental playground for scientists, mathematicians, and engineers who need a reproducible, inspectable way to combine LLMs, program synthesis/evaluation, and theorem proving.

- Who it‚Äôs for
  - Researchers building proof‚Äëof‚Äëconcepts around formal methods (Lean4 today), code scoring/ranking, and agent loops.
  - Engineers who want an OpenAI‚Äëcompatible surface with a local/free stack (Docker) and strong ‚Äúone way to green‚Äù readiness gates.
  - Educators and tinkerers who prefer runnable scenarios over slideware.

- What it gives you
  - Any LLM model that LiteLLM supports: keep your familiar OpenAI‚Äëstyle interface and plug in local/cloud models as needed.
  - Certainly (Lean4 umbrella, beta): take natural language + structured requirements and verify them under Lean4; returns proofs or structured guidance/diagnostics when a requirement doesn‚Äôt compile/prove.
  - CodeWorld: run multiple concurrent algorithmic approaches safely; apply custom, dynamic scoring; rank winners with a built‚Äëin judge.
  - codex‚Äëagent: code‚Äëcentric agent surface (OpenAI‚Äëcompatible) that can run multi‚Äëiteration plans and call MCP tools via your own sidecar/gateway.
  - mini‚Äëagent: a small, deterministic local agent for quick tool‚Äëuse experiments (Python/Rust/Go/JS profiles).
  - Reproducibility by design: deterministic tests (offline) vs live scenarios (skip‚Äëfriendly), strict readiness gates, per‚Äërun artifacts (run_id, request_id, item_ids, session/track).
  - Observability basics: per‚Äërequest IDs, minimal `/metrics`, machine‚Äëreadable manifests for replay.

- What it is not (yet)
  - A hardened production prover/execution sandbox. CodeWorld uses process‚Äëlevel isolation (RLIMITs + optional `unshare -n`) on Linux; containerized workers are recommended for GA.
  - A drop‚Äëin replacement for fully featured theorem proving platforms‚Äîthis is a lightweight bridge for experiments.

Design philosophy
- One way to green: a single readiness flow with strict/live gates for deploy checks.
- Deterministic vs live separation: everything in `tests/` is offline; everything in `scenarios/` can touch the world.
- Local‚Äëfirst: the whole stack can run on your laptop with Docker; cloud providers are optional.

<!-- Removed duplicated TL;DR block to reduce ambiguity; canonical TL;DR lives near top. -->

### What you can do in 5 minutes
- Prove‚Äëaware pipeline: `scenarios/certainly_router_release.py`
- Strategy benchmark: `scenarios/codeworld_judge_live.py`
- Agent loop experiment: mini‚Äëagent or codex‚Äëagent via Router with strict JSON + retries

## Certainly (Lean4 umbrella, beta)

Certainly is the umbrella surface for theorem provers; today it routes to Lean4 only.

- Enable provider: `LITELLM_ENABLE_LEAN4=1` (or `LITELLM_ENABLE_CERTAINLY=1`)
- Bridge: `LEAN4_BRIDGE_BASE` (or `CERTAINLY_BRIDGE_BASE`) defaults to `http://127.0.0.1:8787`
  - Router: use `custom_llm_provider="lean4"` or the alias `"certainly"`
  - Scenarios: `scenarios/lean4_*` and `scenarios/certainly_*` demonstrate both paths

Future backends (e.g., Coq) will plug into the same surface, but are out of scope for this alpha.

<details>
  <summary>Logo variants</summary>
  <p>
    <img src="local/artifacts/logo/SciLLM_balanced_outlined.svg" alt="SciLLM Balanced (default, outlined)" height="36" />
    &nbsp;&nbsp;
    <img src="SciLLM_friendly.svg" alt="SciLLM Friendly" height="36" />
    &nbsp;&nbsp;
    <img src="local/artifacts/logo/SciLLM_icon.svg" alt="SciLLM Icon" height="36" />
    &nbsp;&nbsp;
    <img src="local/artifacts/logo/SciLLM_balanced_dark.svg" alt="SciLLM Balanced Dark" height="36" />
    &nbsp;&nbsp;
    <img src="local/artifacts/logo/SciLLM_balanced_mono.svg" alt="SciLLM Balanced Mono" height="36" />
  </p>
  <p>Use <code>make logo-export</code> to produce outlined SVGs and favicons in <code>local/artifacts/logo/</code>. The generated <code>favicon.ico</code> uses the icon only (no text).</p>
</details>

## Scenarios vs Tests (Determinism Boundary)

- `tests/` are strictly deterministic and offline (no network). Example: Lean4 CLI contract tests in `tests/lean4/`.
- `scenarios/` are live end-to-end demos that may call HTTP bridges or external services. They are skip-friendly when deps aren‚Äôt running.
- New: `scenarios/mcts_codeworld_auto_release.py` prefers a single POST to `/bridge/complete` with `strategy_config.autogenerate`; falls back to a two‚Äëstep flow if generation isn‚Äôt available.

## mini‚Äëagent & codex‚Äëagent (One‚Äëliners)

- mini‚Äëagent (local tools, deterministic): fast, reproducible tool‚Äëuse loop for experiments. Expect final answer + metrics + parsed tool calls.
- codex‚Äëagent (code‚Äëcentric provider): OpenAI‚Äëcompatible sidecar with MCP tools and multi‚Äëiteration plans; health‚Äëcheckable and Router‚Äënative.

Mini‚ÄëAgent (MCP)
- Start the MCP‚Äëstyle mini‚Äëagent locally:
  - `uvicorn litellm.experimental_mcp_client.mini_agent.agent_proxy:app --host 127.0.0.1 --port 8788`
- Probe: `curl -sSf http://127.0.0.1:8788/ready`
- In‚Äëprocess sample: `python examples/mini_agent_inprocess.py` (uses LocalMCPInvoker)
- See also: feature_recipes/MINI_AGENT.md and CONTEXT.md runbook pointers.

## When To Use CodeWorld

Use CodeWorld when you want to evaluate and rank code strategies under your own metrics, with a reproducible manifest and simple HTTP/Router calls.

- MCTS quick calls
  - Alias sugar: `model="codeworld/mcts"` injects `strategy="mcts"`.
  - Autogenerate + MCTS: `model="codeworld/mcts:auto"` (synonym `mcts+auto`) to generate N approaches, then run MCTS. Env overrides: `CODEWORLD_MCTS_AUTO_N`, `CODEWORLD_MCTS_AUTO_TEMPERATURE`, `CODEWORLD_MCTS_AUTO_MODEL`, `CODEWORLD_MCTS_AUTO_MAX_TOKENS`.
  - See [MCTS_CODEWORLD.md](feature_recipes/MCTS_CODEWORLD.md) for details and determinism notes.

- Typical problems
  - Compare competing algorithms (e.g., heuristics vs DP) with identical inputs.
  - Validate repair loops (generate ‚Üí run ‚Üí score ‚Üí keep best) using a deterministic judge.
  - Track improvement plateaus during optimization (optional Redis‚Äëbacked session history).
- What you get
  - Sandbox runner (alpha): executes Python strategies with RLIMITs + AST allow/deny; optional no‚Äënet namespace on Linux.
  - Dynamic scoring: inject `score(task, context, outputs, timings)` to compute domain‚Äëspecific metrics.
  - Judge ranking: built‚Äëin weighted or lexicographic ranking across correctness/speed/brevity.
  - Artifacts: run_manifest with run_id, item_ids, options (session/track), and request_id.
- Try it quickly
  - Bridge: `CODEWORLD_BASE=http://127.0.0.1:8887 python scenarios/codeworld_bridge_release.py`
  - Judge demo (shows speed effect): `python scenarios/codeworld_judge_live.py`

## When To Use Certainly (Lean4)

Use Certainly when you need a light, HTTP‚Äëfriendly bridge to a theorem prover inside LLM/agent workflows.

- Typical problems
  - Batch‚Äëcheck a set of obligations (lemmas) produced by an agent or pipeline.
  - Capture provenance (session/track/run_id) for reproducibility and audit.
  - Keep client code stable while changing the proving backend (Lean4 today).
- What you get
  - Lean4 bridge with canonical `{messages, items, options}` envelope; back‚Äëcompat `lean4_requirements`.
  - Router provider + alias (`custom_llm_provider="lean4"` or `"certainly"`).
  - Artifacts: run_manifest with run_id, item_ids, options (session/track), provider info, and request_id.
- Try it quickly
  - Bridge: `LEAN4_BRIDGE_BASE=http://127.0.0.1:8787 python scenarios/lean4_bridge_release.py`
  - Router alias: `LITELLM_ENABLE_CERTAINLY=1 CERTAINLY_BRIDGE_BASE=http://127.0.0.1:8787 python scenarios/certainly_router_release.py`

Lean4 examples:
- Deterministic tests: `tests/lean4/test_cli_batch.py`, `tests/lean4/test_cli_run.py`.
- Live scenarios: `scenarios/lean4_bridge_release.py`, `scenarios/lean4_bridge_eval_live.py`.
- Optional health test (env-guarded): `tests/lean4/test_bridge_health_optional.py` (runs only if `LEAN4_BRIDGE_BASE` is set).

LiteLLM manages:

## Rename & Compatibility Notice

This repository has been renamed to **scillm** and branded as **SciLLM ‚Äî a scientific/engineering-focused fork of LiteLLM**.

- Core LiteLLM usage remains compatible; Lean4/CodeWorld are optional modules.
- Preferred env flags: `SCILLM_ENABLE_*` (aliases: `LITELLM_ENABLE_*`).
- CLI aliases: `scillm`, `scillm-proxy` (equivalent to litellm commands).
- Deployment profiles: see `docs/deploy/SCILLM_DEPLOY.md` and `deploy/docker/compose.scillm.*.yml`.

- Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints
- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `['choices'][0]['message']['content']`
- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)
- Set Budgets & Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)

[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs) <br>
[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)

Fork Status (our fork)
- See `docs/archive/STATE_OF_PROJECT.md` for a concise, operator‚Äëfriendly status of this fork, including the router_core seam (opt‚Äëin), extras, mini‚Äëagent helper, validation steps, CI, and roadmap.

Fork Quick Start (Recap)
1. `make run-scenarios`
2. (Optional) `docker compose -f local/docker/compose.agents.yml up --build -d` for mini + codex endpoints
3. See [QUICKSTART.md](QUICKSTART.md) for scenario commands
4. Mini‚ÄëAgent docs: `docs/my-website/docs/experimental/mini-agent.md`
5. Project status: `docs/archive/STATE_OF_PROJECT.md`


üö® **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)

Support for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.yml&title=%5BFeature%5D%3A+).

# Usage ([**Docs**](https://docs.litellm.ai/docs/))

> [!IMPORTANT]
> LiteLLM v1.0.0 now requires `openai>=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)
> LiteLLM v1.40.14+ now requires `pydantic>=2.0.0`. No changes required.

<a target="_blank" href="https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

```shell
pip install litellm
```

```python
from litellm import completion
import os

## set ENV variables
os.environ["OPENAI_API_KEY"] = "your-openai-key"
os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-key"

messages = [{ "content": "Hello, how are you?","role": "user"}]

# openai call
response = completion(model="openai/gpt-4o", messages=messages)

# anthropic call
response = completion(model="anthropic/claude-sonnet-4-20250514", messages=messages)
print(response)
```

### Response (OpenAI Format)

```json
{
    "id": "chatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de",
    "created": 1751494488,
    "model": "claude-sonnet-4-20250514",
    "object": "chat.completion",
    "system_fingerprint": null,
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "Hello! I'm doing well, thank you for asking. I'm here and ready to help with whatever you'd like to discuss or work on. How are you doing today?",
                "role": "assistant",
                "tool_calls": null,
                "function_call": null
            }
        }
    ],
    "usage": {
        "completion_tokens": 39,
        "prompt_tokens": 13,
        "total_tokens": 52,
        "completion_tokens_details": null,
        "prompt_tokens_details": {
            "audio_tokens": null,
            "cached_tokens": 0
        },
        "cache_creation_input_tokens": 0,
        "cache_read_input_tokens": 0
    }
}
```

Call any model supported by a provider, with `model=<provider_name>/<model_name>`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)

OpenAI‚Äëcompatible endpoints (e.g., Chutes) ‚Äî model IDs
- Use vendor‚Äëfirst IDs returned by `/v1/models` (e.g., `moonshotai/Kimi-K2-Instruct-0905`).
- You may pass `custom_llm_provider="openai"`, but scillm now defaults to 'openai'
  when `api_base` points at an OpenAI‚Äëcompatible gateway (e.g., `CHUTES_API_BASE`).
- Avoid `openai/` prefixes for model names; if present, they are stripped automatically
  for OpenAI‚Äëcompatible providers.


Strict JSON (auto sanitize)
- Set `SCILLM_JSON_SANITIZE=1` or pass `auto_json_sanitize=True` to normalize JSON-mode responses (removes fences/prose, validates JSON).
- Recommended with `response_format={"type":"json_object"}` or `response_mime_type="application/json"`.
Session preflight (optional)
- Cache model IDs once and enable guard to validate IDs locally for the session (no extra network per call).
```python
from litellm.extras.preflight import preflight_models
import os
preflight_models(api_base=os.environ["CHUTES_API_BASE"], api_key=os.environ.get("CHUTES_API_KEY"))
# export SCILLM_MODEL_PREFLIGHT=1
# Optional: map doc-style names to live canonical IDs
# export SCILLM_MODEL_ALIAS=1  # fail fast on unknown IDs using cached catalog
```

Python example (no prefix required):

With aliasing (default) and cutoff override:
```python
from scillm import completion; import os
print(completion(model='mistral-ai/Mistral-Small-3.2-24B', messages=[{'role':'user','content':'{}'}], response_format={'type':'json_object'}, api_base=os.environ['CHUTES_API_BASE'], api_key=os.environ.get('CHUTES_API_KEY',''), fallback_closest=True, fallback_closest_cutoff=0.58).choices[0].message.content)
```
```python
from scillm import completion; import os
r = completion(model='moonshotai/Kimi-K2-Instruct-0905',
              messages=[{'role':'user','content':'{}'}],
              response_format={'type':'json_object'},
              api_base=os.environ['CHUTES_API_BASE'],
              api_key=os.environ.get('CHUTES_API_KEY',''))
print(r.choices[0].message.content)
```

## Async ([Docs](https://docs.litellm.ai/docs/completion/stream#async-completion))

```python
from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = "Hello, how are you?"
    messages = [{"content": user_message, "role": "user"}]
    response = await acompletion(model="openai/gpt-4o", messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
```

## Streaming ([Docs](https://docs.litellm.ai/docs/completion/stream))

liteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.
Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

```python
from litellm import completion
response = completion(model="openai/gpt-4o", messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or "")

# claude sonnet 4
response = completion('anthropic/claude-sonnet-4-20250514', messages, stream=True)
for part in response:
    print(part)
```

### Response chunk (OpenAI Format)

```json
{
    "id": "chatcmpl-fe575c37-5004-4926-ae5e-bfbc31f356ca",
    "created": 1751494808,
    "model": "claude-sonnet-4-20250514",
    "object": "chat.completion.chunk",
    "system_fingerprint": null,
    "choices": [
        {
            "finish_reason": null,
            "index": 0,
            "delta": {
                "provider_specific_fields": null,
                "content": "Hello",
                "role": "assistant",
                "function_call": null,
                "tool_calls": null,
                "audio": null
            },
            "logprobs": null
        }
    ],
    "provider_specific_fields": null,
    "stream_options": null,
    "citations": null
}
```

## Logging Observability ([Docs](https://docs.litellm.ai/docs/observability/callbacks))

LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack

```python
from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key"
os.environ["HELICONE_API_KEY"] = "your-helicone-auth-key"
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""
os.environ["ATHINA_API_KEY"] = "your-athina-api-key"

os.environ["OPENAI_API_KEY"] = "your-openai-key"

# set callbacks
litellm.success_callback = ["lunary", "mlflow", "langfuse", "athina", "helicone"] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model="openai/gpt-4o", messages=[{"role": "user", "content": "Hi üëã - i'm openai"}])
```

# LiteLLM Proxy Server (LLM Gateway) - ([Docs](https://docs.litellm.ai/docs/simple_proxy))

Track spend + Load Balance across multiple projects

[Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted)

The proxy provides:

1. [Hooks for auth](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)
2. [Hooks for logging](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class)
3. [Cost tracking](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend)
4. [Rate Limiting](https://docs.litellm.ai/docs/proxy/users#set-rate-limits)

## üìñ Proxy Endpoints - [Swagger Docs](https://litellm-api.up.railway.app/)


## Quick Start Proxy - CLI

```shell
pip install 'litellm[proxy]'
```

### Step 1: Start litellm proxy

```shell
$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
```

### Step 2: Make ChatCompletions Request to Proxy


> [!IMPORTANT]
> üí° [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)

```python
import openai # openai v1.0.0+
client = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [
    {
        "role": "user",
        "content": "this is a test request, write a short poem"
    }
])

print(response)
```

## Proxy Key Management ([Docs](https://docs.litellm.ai/docs/proxy/virtual_keys))

Connect the proxy with a Postgres DB to create proxy keys

```bash
# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo 'LITELLM_MASTER_KEY="sk-1234"' > .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/
# password generator to get a random hash for litellm salt key
echo 'LITELLM_SALT_KEY="sk-1234"' >> .env

source .env

# Start
docker-compose up
```


UI on `/ui` on your proxy server
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)

Set budgets and rate limits across multiple projects
`POST /key/generate`

### Request

```shell
curl 'http://0.0.0.0:4000/key/generate' \
--header 'Authorization: Bearer sk-1234' \
--header 'Content-Type: application/json' \
--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4", "claude-2"], "duration": "20m","metadata": {"user": "ishaan@berri.ai", "team": "core-infra"}}'
```

### Expected Response

```shell
{
    "key": "sk-kdEXbIqZRwEeEiHwdg7sFA", # Bearer token
    "expires": "2023-11-19T01:38:25.838000+00:00" # datetime object
}
```

## Supported Providers ([Docs](https://docs.litellm.ai/docs/providers))

| Provider                                                                            | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Async Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Async Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Async Embedding](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Async Image Generation](https://docs.litellm.ai/docs/image_generation) |
|-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------|
| [openai](https://docs.litellm.ai/docs/providers/openai)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [Meta - Llama API](https://docs.litellm.ai/docs/providers/meta_llama)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                              |                                                                        |
| [azure](https://docs.litellm.ai/docs/providers/azure)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [AI/ML API](https://docs.litellm.ai/docs/providers/aiml)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker)             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock)                     | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex)                 | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [google - palm](https://docs.litellm.ai/docs/providers/palm)                        | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [google AI Studio - gemini](https://docs.litellm.ai/docs/providers/gemini)          | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral)                    | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers)  | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [CompactifAI](https://docs.litellm.ai/docs/providers/compactifai)                   | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [cohere](https://docs.litellm.ai/docs/providers/cohere)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [anthropic](https://docs.litellm.ai/docs/providers/anthropic)                       | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [empower](https://docs.litellm.ai/docs/providers/empower)                    | ‚úÖ                                                      | ‚úÖ                                                                              | ‚úÖ                                                                                  | ‚úÖ                                                                                |
| [huggingface](https://docs.litellm.ai/docs/providers/huggingface)                   | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [replicate](https://docs.litellm.ai/docs/providers/replicate)                       | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [together_ai](https://docs.litellm.ai/docs/providers/togetherai)                    | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [openrouter](https://docs.litellm.ai/docs/providers/openrouter)                     | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [ai21](https://docs.litellm.ai/docs/providers/ai21)                                 | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [baseten](https://docs.litellm.ai/docs/providers/baseten)                           | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [vllm](https://docs.litellm.ai/docs/providers/vllm)                                 | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [nlp_cloud](https://docs.litellm.ai/docs/providers/nlp_cloud)                       | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [aleph alpha](https://docs.litellm.ai/docs/providers/aleph_alpha)                   | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [petals](https://docs.litellm.ai/docs/providers/petals)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [ollama](https://docs.litellm.ai/docs/providers/ollama)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [deepinfra](https://docs.litellm.ai/docs/providers/deepinfra)                       | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [perplexity-ai](https://docs.litellm.ai/docs/providers/perplexity)                  | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [Groq AI](https://docs.litellm.ai/docs/providers/groq)                              | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [Deepseek](https://docs.litellm.ai/docs/providers/deepseek)                         | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [anyscale](https://docs.litellm.ai/docs/providers/anyscale)                         | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [IBM - watsonx.ai](https://docs.litellm.ai/docs/providers/watsonx)                  | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [voyage ai](https://docs.litellm.ai/docs/providers/voyage)                          |                                                         |                                                                                 |                                                                                     |                                                                                   | ‚úÖ                                                                             |                                                                         |
| [xinference [Xorbits Inference]](https://docs.litellm.ai/docs/providers/xinference) |                                                         |                                                                                 |                                                                                     |                                                                                   | ‚úÖ                                                                             |                                                                         |
| [FriendliAI](https://docs.litellm.ai/docs/providers/friendliai)                              | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [Galadriel](https://docs.litellm.ai/docs/providers/galadriel)                              | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [GradientAI](https://docs.litellm.ai/docs/providers/gradient_ai)                              | ‚úÖ                                                       | ‚úÖ                                                                               |                                                                                   |                                                                                  |                                                                               |                                                                         |
| [Novita AI](https://novita.ai/models/llm?utm_source=github_litellm&utm_medium=github_readme&utm_campaign=github_link)                     | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [Featherless AI](https://docs.litellm.ai/docs/providers/featherless_ai)                              | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [Nebius AI Studio](https://docs.litellm.ai/docs/providers/nebius)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [Heroku](https://docs.litellm.ai/docs/providers/heroku)                             | ‚úÖ                                                       | ‚úÖ                                                                               |                                                                                    |                                                                                  |                                                                              |                                                                         |
| [OVHCloud AI Endpoints](https://docs.litellm.ai/docs/providers/ovhcloud)                             | ‚úÖ                                                       | ‚úÖ                                                                               |                                                                                    |                                                                                  |                                                                              |                                                                         |

[**Read the Docs**](https://docs.litellm.ai/docs/)

## Contributing

Interested in contributing? Contributions to LiteLLM Python SDK, Proxy Server, and LLM integrations are both accepted and highly encouraged!

**Quick start:** `git clone` ‚Üí `make install-dev` ‚Üí `make format` ‚Üí `make lint` ‚Üí `make test-unit`

See our comprehensive [Contributing Guide (CONTRIBUTING.md)](CONTRIBUTING.md) for detailed instructions.

# Enterprise
For companies that need better security, user management and professional support

[Talk to founders](https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat)

This covers:
- ‚úÖ **Features under the [LiteLLM Commercial License](https://docs.litellm.ai/docs/proxy/enterprise):**
- ‚úÖ **Feature Prioritization**
- ‚úÖ **Custom Integrations**
- ‚úÖ **Professional Support - Dedicated discord + slack**
- ‚úÖ **Custom SLAs**
- ‚úÖ **Secure access with Single Sign-On**

# Contributing

We welcome contributions to LiteLLM! Whether you're fixing bugs, adding features, or improving documentation, we appreciate your help.

## Quick Start for Contributors

This requires poetry to be installed.

```bash
git clone https://github.com/BerriAI/litellm.git
cd litellm
make install-dev    # Install development dependencies
make format         # Format your code
make lint           # Run all linting checks
make test-unit      # Run unit tests
make format-check   # Check formatting only
```

For detailed contributing guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md).

## Code Quality / Linting

LiteLLM follows the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html).

Our automated checks include:
- **Black** for code formatting
- **Ruff** for linting and code quality
- **MyPy** for type checking
- **Circular import detection**
- **Import safety checks**


All these checks must pass before your PR can be merged.


# Support / talk with founders

- [Schedule Demo üëã](https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version)
- [Community Discord üí≠](https://discord.gg/wuPM9dRgDw)
- [Community Slack üí≠](https://www.litellm.ai/support)
- Our numbers üìû +1 (770) 8783-106 / ‚Ä≠+1 (412) 618-6238‚Ä¨
- Our emails ‚úâÔ∏è ishaan@berri.ai / krrish@berri.ai

# Why did we build this

- **Need for simplicity**: Our code started to get extremely complicated managing & translating calls between Azure, OpenAI and Cohere.

# Contributors

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

<a href="https://github.com/BerriAI/litellm/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=BerriAI/litellm" />
</a>


## Run in Developer mode
### Services
1. Setup .env file in root
2. Run dependant services `docker-compose up db prometheus`

### Backend
1. (In root) create virtual environment `python -m venv .venv`
2. Activate virtual environment `source .venv/bin/activate`
3. Install dependencies `pip install -e ".[all]"`
4. Start proxy backend `python3 /path/to/litellm/proxy_cli.py`

### Frontend
1. Navigate to `ui/litellm-dashboard`
2. Install dependencies `npm install`
3. Run `npm run dev` to start the dashboard


## Operations
- Canary runbook: local/docs/02_operational/CANARY_PARITY_PLAN.md
- Stress testing: local/docs/02_operational/STRESS_TESTING.md
- Parity scripts: local/scripts/router_core_parity.py, local/scripts/parity_summarize.py (use `uv run`)
- CI (live, secrets-gated): Nightly Parity & Stress, Weekly Streaming Stress, and Manual Stress workflows (see badges above).
## Reviews

See docs/reviews/ for current review briefs:
- docs/reviews/REVIEW_REQUEST_SCILLM.md
- docs/reviews/REVIEW_REQUEST_CERTAINLY.md

## Architecture (one picture)

See docs/architecture/overview.mmd for a Mermaid diagram of the main flow:

Router ‚Üí Bridges (CodeWorld, Certainly) ‚Üí Runners/Prover ‚Üí Manifests/Artifacts ‚Üí Health/Metrics

## Repo Layout

- `deploy/docker/` ‚Äî Tracked Dockerfiles and compose profiles (core, modules, full, stack).
- `docs/` ‚Äî Project documentation (deploy guides, reviews, archive status, assets/screenshots).
- `litellm/` ‚Äî Providers and Router integration (adds `codeworld`, `lean4`, and `certainly` alias).
- `src/` ‚Äî Bridges and engines:
  - `codeworld/bridge` and `codeworld/engine` (strategy/scoring runners, judge).
  - `lean4_prover/bridge` (Certainly/Lean4 bridge).
- `scenarios/` ‚Äî Live end‚Äëto‚Äëend demos (skip‚Äëfriendly) for bridges and Router.
- `tests/` ‚Äî Deterministic, offline unit tests; `tests/_archive/` for legacy root tests.
- `local/artifacts/` ‚Äî Generated artifacts (logo, MVP reports, run JSON). CI uploads artifacts from here.
- `scripts/` ‚Äî Top‚Äëlevel utility scripts; provider parity/report helpers.

## Competitive Positioning (quick read)

| Use SciLLM when‚Ä¶ | Use other tools when‚Ä¶ |
| --- | --- |
| You need a prover‚Äëin‚Äëthe‚Äëloop stack with reproducible artifacts, strict readiness, and an OpenAI‚Äëcompatible surface. | You only need a provider gateway (LiteLLM upstream) or agent composition without proofs (LangChain/LlamaIndex/AutoGen). |
| You want to execute & rank code strategies under custom metrics with a safety wrapper and built‚Äëin judge. | You only need LLM output scoring (DeepEval/Langfuse/OpenAI Evals) without running arbitrary code variants. |
| You want local‚Äëfirst bring‚Äëup (Docker), health/metrics endpoints, and per‚Äërun manifests for audit. | You prefer hosted agents or cloud‚Äëonly flows and don‚Äôt need strict reproducibility gates. |

## Beta Limits & Policy

- Platforms: Linux recommended. macOS/Windows run with reduced isolation (no `unshare -n`).
- CodeWorld isolation: process‚Äëlevel (RLIMITs + AST allow/deny). For GA, use containerized workers (seccomp/AppArmor).
- Certainly (Lean4) status: umbrella provider in beta; future Coq/Isabelle backends are out of scope for this milestone.
- Deprecation: `additional_kwargs['certainly']` is canonical; mirroring to `['lean4']` defaults to 1 for this release and flips to 0 next release.

## LLMs Are Fallible ‚Äî Verify Deterministically

Scientists and engineers are rightly skeptical of hallucinations. SciLLM is designed to ‚Äútrust, but verify‚Äù:
- Treat LLM outputs like untrusted suggestions; ask them to produce artifacts that can be checked.
- Verify with compiler‚Äëlike determinism: CodeWorld executes strategies under limits and scores them; Certainly compiles and proves Lean4 obligations.
- Keep a paper trail: every run emits a manifest (run_id, request_id, item_ids, session/track, provider info) for replay and audit.
- Separate concerns: deterministic unit tests live in `tests/`; live scenarios in `scenarios/` can touch the world and are skip‚Äëfriendly.

## Operator Checklist

- Set `READINESS_LIVE=1` and `STRICT_READY=1` for strict runs; choose `READINESS_EXPECT=codeworld,certainly` (or `codeworld,lean4`).
- Check `/healthz` and `/metrics` on both bridges.
- Find run artifacts under `local/artifacts/runs/` (includes `run_id`, `request_id`, `item_id`s, session/track, provider info).
- Gate CI on judge thresholds (% proved, correctness/speed) and return artifacts for review.
  - Model discovery: example ids like ‚Äúgpt‚Äë5‚Äù may not be present; always choose an id from `GET $CODEX_AGENT_API_BASE/v1/models`.
  - The reasoning flag (`reasoning={"effort":"high"}`) is optional; examples include it to demonstrate high‚Äëreasoning flows.

## Security & Isolation Notes
- CodeWorld: process RLIMITs + optional `unshare -n`. Use containers for stronger isolation.
- codex‚Äëagent echo mode: for development only; disable before adding real credentials.
- Validate outputs (proof artifacts, code variants) before trusting or executing downstream.

## Parallel Fan‚ÄëOut (Advanced)
Use `parallel_acompletions` (ordered) for stable batch results. Experimental `parallel_as_completed` yields as each finishes (unordered); prefer first in pipelines requiring deterministic mapping.

```python
from litellm import Router
from litellm.router_utils.parallel_acompletion import RouterParallelRequest
import os

router = Router(model_list=[{
  "model_name":"demo",
  "litellm_params":{
    "model":"<MODEL_ID>",
    "custom_llm_provider":"codex-agent",
    "api_base":os.getenv("CODEX_AGENT_API_BASE")
  }
}])

reqs = [
  RouterParallelRequest("demo", [{"role":"user","content":"List 3 primes"}]),
  RouterParallelRequest("demo", [{"role":"user","content":"Say hi"}])
]

results = await router.parallel_acompletions(reqs, concurrency=2)
for r in results:
  print(r.index, bool(r.error), r.content)
```

---
End of README refinements (duplicate TL;DR removed, placeholders clarified, security & parallel sections added).

## Automatic Chutes Selection, Fallbacks, and Attribution (opt‚Äëin)

SciLLM ships friction‚Äëfree helpers for OpenAI‚Äëcompatible gateways like Chutes:
- `auto_router_from_env(kind="text", require_json=True)` ‚Üí builds a Router from numbered chutes (`CHUTES_API_BASE_n/CHUTES_API_KEY_n`) and orders by availability (`/v1/models`) and utilization (`/chutes/utilization`, advisory).
- `infer_with_fallback(messages, kind="text", require_json=True)` ‚Üí returns the first successful response and attaches `resp.scillm_meta` with `served_model`, `routing`, and `attempts`.
- `find_best_chutes_model(kind="text", require_json=True, util_threshold=0.85)` ‚Üí picks a single Router‚Äëready entry under the threshold when possible.
- `warm_chutes_caches(model_list)` ‚Üí one‚Äëshot preflight to warm `/v1/models` and utilization caches (TTL).

Environment (no code edits)
- Provide numbered chutes: `CHUTES_API_BASE_1`, `CHUTES_API_KEY_1`, `CHUTES_API_BASE_2`, `CHUTES_API_KEY_2`, ‚Ä¶
- Optional per‚Äëkind model pins: `CHUTES_TEXT_MODEL_1`, `CHUTES_VLM_MODEL_1`, `CHUTES_TOOLS_MODEL_1`
- Ranker knobs: `SCILLM_UTIL_TTL_S=45`, `SCILLM_UTIL_HI=0.85`, `SCILLM_UTIL_LO=0.50`, `SCILLM_UTIL_K=2`

Live notebooks (generated by `scripts/notebooks_build.py`)
- `09_fallback_infer_with_meta.ipynb` ‚Äî fallback + attribution
- `10_auto_router_one_liner.ipynb` ‚Äî one‚Äëliner Router
- `11_provider_perplexity.ipynb` ‚Äî Litellm‚Äënative provider
- `12_provider_openai.ipynb` / `13_provider_anthropic.ipynb` ‚Äî normal providers
- `14_provider_matrix.ipynb` ‚Äî OpenAI, Anthropic, Perplexity, and Chutes (with attribution) side‚Äëby‚Äëside
\n+## Notebooks Quick Guide

Each notebook below is self‚Äëcontained and runnable. Use the ‚Äúexecuted‚Äù versions to see verified outputs; use the source versions to modify and re‚Äërun locally.

- 01 Chutes ‚Äî OpenAI‚ÄëCompatible
  - Source: `notebooks/01_chutes_openai_compatible.ipynb` ‚Äî When you want a single JSON response via Chutes (Bearer header), quick sanity for pipelines.
  - Executed: `notebooks/executed/01_chutes_openai_compatible_executed.ipynb`

- 02 Router.parallel_acompletions
  - Source: `notebooks/02_router_parallel_batch.ipynb` ‚Äî When you need fast, concurrent batch calls; shows per‚Äëitem timeouts and loop‚Äësafe async.
  - Executed: `notebooks/executed/02_router_parallel_batch_executed.ipynb`

- 03 Model List ‚Äî First Success
  - Source: `notebooks/03_model_list_first_success.ipynb` ‚Äî Prefer primary then auto‚Äëfallback; per‚Äëdeployment headers included.
  - Executed: `notebooks/executed/03_model_list_first_success_executed.ipynb`

- 04a Advanced ‚Äî Tools (smoke‚Äësafe)
  - Source: `notebooks/04a_tools_only.ipynb` ‚Äî When you need function/tool calling with OpenAI‚Äëformat tools; includes rate‚Äëlimit tips.
  - Executed: `notebooks/04a_tools_only_executed.ipynb`

- 04b Advanced ‚Äî Streaming (manual)
  - Source: `notebooks/04b_streaming_demo.ipynb` ‚Äî Live token streaming demo; set `SCILLM_ALLOW_STREAM_SMOKE=1` to execute.

- 05 Codex‚ÄëAgent ‚Äî Doctor
  - Source: `notebooks/05_codex_agent_doctor.ipynb` ‚Äî Verifies health and a minimal JSON chat through the Codex agent.
  - Executed: `notebooks/executed/05_codex_agent_doctor_executed.ipynb`

- 06 Mini‚ÄëAgent ‚Äî Doctor
  - Source: `notebooks/06_mini_agent_doctor.ipynb` ‚Äî Validates mini‚Äëagent surfaces and prints a minimal JSON chat.
  - Executed: `notebooks/executed/06_mini_agent_doctor_executed.ipynb`

- 07 CodeWorld ‚Äî MCTS
  - Source: `notebooks/07_codeworld_mcts.ipynb` ‚Äî Starts the bridge and runs an MCTS scenario; expect `run_manifest.mcts_stats`.
  - Executed: `notebooks/executed/07_codeworld_mcts_executed.ipynb`

- 08 Certainly Bridge ‚Äî Doctor
  - Source: `notebooks/08_certainly_bridge.ipynb` ‚Äî Confirms the bridge responds and proves ‚â•1 item; includes quick triage.
  - Executed: `notebooks/executed/08_certainly_bridge_executed.ipynb`

- 09 Fallback Inference + Attribution
  - Source: `notebooks/09_fallback_infer_with_meta.ipynb` ‚Äî Reliability path; returns first success and `served_model` in meta.
  - Executed: `notebooks/executed/09_fallback_infer_with_meta_executed.ipynb`

- 10 Auto Router ‚Äî One Liner
  - Source: `notebooks/10_auto_router_one_liner.ipynb` ‚Äî Builds a Router from env (availability + utilization) and routes JSON.
  - Executed: `notebooks/executed/10_auto_router_one_liner_executed.ipynb`

- 11 Perplexity ‚Äî Sonar Family
  - Source: `notebooks/11_provider_perplexity.ipynb` ‚Äî Sonar model guide + live model list; examples for sync/async/batch; default `sonar`.
  - Executed: `notebooks/executed/11_provider_perplexity_executed.ipynb`

- 12 Provider ‚Äî OpenAI
  - Source: `notebooks/12_provider_openai.ipynb` ‚Äî Baseline OpenAI usage via SciLLM; simple ‚ÄúOK‚Äù sanity.
  - Executed: `notebooks/executed/12_provider_openai_executed.ipynb`

- 13 Provider ‚Äî Anthropic
  - Source: `notebooks/13_provider_anthropic.ipynb` ‚Äî Baseline Claude usage via SciLLM; error‚Äësafe when key missing.
  - Executed: `notebooks/executed/13_provider_anthropic_executed.ipynb`

- 14 Provider Matrix ‚Äî OpenAI, Anthropic, Perplexity, Chutes
  - Source: `notebooks/14_provider_matrix.ipynb` ‚Äî First‚Äëtime environment triage; each section skips when its key is missing.
  - Executed: `notebooks/executed/14_provider_matrix_executed.ipynb`
